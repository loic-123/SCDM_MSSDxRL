{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MSSD x RL: Full Experiment Pipeline\n",
    "\n",
    "This notebook runs the **complete MSSD (Multi-Scale Sequential Shift Detector)** pipeline end-to-end:\n",
    "\n",
    "1. **Install dependencies** and verify GPU availability\n",
    "2. **Train RL agents** (Tabular Q-learning for CliffWalking, DQN for CartPole)\n",
    "3. **Collect reference observations** from the trained agents under nominal conditions\n",
    "4. **Run the unit tests** to validate all components\n",
    "5. **Run a smoke test** (quick 2-trial sweep) to verify the pipeline works end-to-end\n",
    "6. **Run full experiment sweeps** (50 trials x 3 shift types x 3 severities per environment)\n",
    "7. **Generate figures and LaTeX tables** for the report\n",
    "\n",
    "---\n",
    "\n",
    "## Project Overview\n",
    "\n",
    "MSSD wraps a deployed RL agent with a **runtime monitor** that diagnoses *which aspect* of the observation distribution has shifted:\n",
    "\n",
    "| Probe | What it detects | Statistic |\n",
    "|-------|----------------|----------|\n",
    "| **Body** | Mean/variance shift | MMD (d>4) or max-KS (d<=4) |\n",
    "| **Tail** | Rare dangerous states | CVaR_0.95 difference |\n",
    "| **Structure** | Correlation breakdown | Frobenius norm of correlation diff |\n",
    "\n",
    "Each probe's test statistic is converted to an **e-value** via block-bootstrap permutation, then accumulated in a **product martingale**. An alarm fires when the combined log-wealth exceeds `log(1/alpha)`, and the probe with the highest log-wealth provides the **diagnosis**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Setup\n",
    "\n",
    "Make sure you are running this notebook from the project root directory, or adjust the paths below.\n",
    "\n",
    "If you have a **conda environment** with GPU-enabled PyTorch (e.g., `torch_5070`), activate it before launching Jupyter:\n",
    "```bash\n",
    "conda activate torch_5070\n",
    "jupyter notebook\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project root: d:\\loic\\ETUDE\\IMPERIAL\\cours\\Spring Term\\ML for Safety Critical Decision-Making - ELEC70122\\Safety Critical Group project\\final_project_idea\n",
      "Working directory: d:\\loic\\ETUDE\\IMPERIAL\\cours\\Spring Term\\ML for Safety Critical Decision-Making - ELEC70122\\Safety Critical Group project\\final_project_idea\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Set the project root (adjust if needed)\n",
    "PROJECT_ROOT = Path(os.getcwd()).parent if Path(os.getcwd()).name == \"notebooks\" else Path(os.getcwd())\n",
    "os.chdir(PROJECT_ROOT)\n",
    "sys.path.insert(0, str(PROJECT_ROOT / \"src\"))\n",
    "\n",
    "print(f\"Project root: {PROJECT_ROOT}\")\n",
    "print(f\"Working directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.3 -> 26.0.1\n",
      "[notice] To update, run: C:\\Program Files\\Python311\\python.exe -m pip install --upgrade pip\n",
      "ERROR: Exception:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\loicb\\AppData\\Roaming\\Python\\Python311\\site-packages\\pip\\_internal\\cli\\base_command.py\", line 107, in _run_wrapper\n",
      "    status = _inner_run()\n",
      "             ^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\loicb\\AppData\\Roaming\\Python\\Python311\\site-packages\\pip\\_internal\\cli\\base_command.py\", line 98, in _inner_run\n",
      "    return self.run(options, args)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\loicb\\AppData\\Roaming\\Python\\Python311\\site-packages\\pip\\_internal\\cli\\req_command.py\", line 85, in wrapper\n",
      "    return func(self, options, args)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\loicb\\AppData\\Roaming\\Python\\Python311\\site-packages\\pip\\_internal\\commands\\install.py\", line 388, in run\n",
      "    requirement_set = resolver.resolve(\n",
      "                      ^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\loicb\\AppData\\Roaming\\Python\\Python311\\site-packages\\pip\\_internal\\resolution\\resolvelib\\resolver.py\", line 79, in resolve\n",
      "    collected = self.factory.collect_root_requirements(root_reqs)\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\loicb\\AppData\\Roaming\\Python\\Python311\\site-packages\\pip\\_internal\\resolution\\resolvelib\\factory.py\", line 538, in collect_root_requirements\n",
      "    reqs = list(\n",
      "           ^^^^^\n",
      "  File \"C:\\Users\\loicb\\AppData\\Roaming\\Python\\Python311\\site-packages\\pip\\_internal\\resolution\\resolvelib\\factory.py\", line 494, in _make_requirements_from_install_req\n",
      "    cand = self._make_base_candidate_from_link(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\loicb\\AppData\\Roaming\\Python\\Python311\\site-packages\\pip\\_internal\\resolution\\resolvelib\\factory.py\", line 205, in _make_base_candidate_from_link\n",
      "    self._editable_candidate_cache[link] = EditableCandidate(\n",
      "                                           ^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\loicb\\AppData\\Roaming\\Python\\Python311\\site-packages\\pip\\_internal\\resolution\\resolvelib\\candidates.py\", line 343, in __init__\n",
      "    super().__init__(\n",
      "  File \"C:\\Users\\loicb\\AppData\\Roaming\\Python\\Python311\\site-packages\\pip\\_internal\\resolution\\resolvelib\\candidates.py\", line 161, in __init__\n",
      "    self.dist = self._prepare()\n",
      "                ^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\loicb\\AppData\\Roaming\\Python\\Python311\\site-packages\\pip\\_internal\\resolution\\resolvelib\\candidates.py\", line 238, in _prepare\n",
      "    dist = self._prepare_distribution()\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\loicb\\AppData\\Roaming\\Python\\Python311\\site-packages\\pip\\_internal\\resolution\\resolvelib\\candidates.py\", line 353, in _prepare_distribution\n",
      "    return self._factory.preparer.prepare_editable_requirement(self._ireq)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\loicb\\AppData\\Roaming\\Python\\Python311\\site-packages\\pip\\_internal\\operations\\prepare.py\", line 714, in prepare_editable_requirement\n",
      "    dist = _get_prepared_distribution(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\loicb\\AppData\\Roaming\\Python\\Python311\\site-packages\\pip\\_internal\\operations\\prepare.py\", line 77, in _get_prepared_distribution\n",
      "    abstract_dist.prepare_distribution_metadata(\n",
      "  File \"C:\\Users\\loicb\\AppData\\Roaming\\Python\\Python311\\site-packages\\pip\\_internal\\distributions\\sdist.py\", line 53, in prepare_distribution_metadata\n",
      "    self.req.editable_sanity_check()\n",
      "  File \"C:\\Users\\loicb\\AppData\\Roaming\\Python\\Python311\\site-packages\\pip\\_internal\\req\\req_install.py\", line 501, in editable_sanity_check\n",
      "    if self.editable and not self.supports_pyproject_editable:\n",
      "                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Program Files\\Python311\\Lib\\functools.py\", line 1001, in __get__\n",
      "    val = self.func(instance)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\loicb\\AppData\\Roaming\\Python\\Python311\\site-packages\\pip\\_internal\\req\\req_install.py\", line 237, in supports_pyproject_editable\n",
      "    return \"build_editable\" in self.pep517_backend._supported_features()\n",
      "                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\loicb\\AppData\\Roaming\\Python\\Python311\\site-packages\\pip\\_vendor\\pyproject_hooks\\_impl.py\", line 180, in _supported_features\n",
      "    return self._call_hook(\"_supported_features\", {})\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\loicb\\AppData\\Roaming\\Python\\Python311\\site-packages\\pip\\_vendor\\pyproject_hooks\\_impl.py\", line 402, in _call_hook\n",
      "    raise BackendUnavailable(\n",
      "pip._vendor.pyproject_hooks._impl.BackendUnavailable: Cannot import 'setuptools.backends._legacy'\n"
     ]
    }
   ],
   "source": [
    "# Install the package in editable mode (run once)\n",
    "!pip install -e . -q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check GPU availability\n",
    "\n",
    "DQN training for CartPole benefits from GPU acceleration. The `device: \"auto\"` setting in `configs/defaults.yaml` will automatically use CUDA if available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.10.0.dev20251204+cu130\n",
      "CUDA available: True\n",
      "GPU: NVIDIA GeForce RTX 5070 Ti Laptop GPU\n",
      "CUDA version: 13.0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"CUDA version: {torch.version.cuda}\")\n",
    "else:\n",
    "    print(\"No GPU detected - training will use CPU (slower for DQN, fine for Tabular Q).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create output directories\n",
    "\n",
    "All artifacts (trained agents, reference data, experiment results, figures) are stored under `artifacts/`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from pathlib import Path\n\ndirs = [\n    \"artifacts/agents/cliffwalking\",\n    \"artifacts/agents/cartpole\",\n    \"artifacts/reference\",\n    \"artifacts/results\",\n    \"artifacts/figures\",\n    \"artifacts/tables\",\n    \"artifacts/logs\",\n]\nfor d in dirs:\n    Path(d).mkdir(parents=True, exist_ok=True)\n\nprint(\"Output directories created.\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. Train RL Agents\n",
    "\n",
    "We train two agents:\n",
    "\n",
    "### 1a. Tabular Q-learning for CliffWalking-v1\n",
    "\n",
    "- **Environment**: `CliffWalking-v1` wrapped with `CliffWalkingContinuousObs` to produce 3D observations `(row, col, cliff_distance)` instead of a single integer state.\n",
    "- **Algorithm**: Tabular Q-learning with epsilon-greedy exploration.\n",
    "- **Episodes**: 5,000 training episodes.\n",
    "- **Output**: Q-table saved as `.npy` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "!python scripts/train_agent.py --config configs/envs/cliffwalking.yaml --output artifacts/agents/cliffwalking/q_table.npy --seed 42"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1b. DQN for CartPole-v1\n",
    "\n",
    "- **Environment**: `CartPole-v1` with 4D native observations `(x, x_dot, theta, theta_dot)`.\n",
    "- **Algorithm**: Deep Q-Network with experience replay and target network.\n",
    "- **Episodes**: 500 training episodes.\n",
    "- **Device**: Automatically uses GPU if available (set via `--device auto`).\n",
    "- **Output**: PyTorch checkpoint saved as `.pt` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "!python scripts/train_agent.py --config configs/envs/cartpole.yaml --output artifacts/agents/cartpole/dqn_checkpoint.pt --seed 42 --device auto"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Collect Reference Observations\n",
    "\n",
    "We deploy each trained agent in the **nominal (unshifted)** environment and collect observations. These serve as the **reference distribution** against which the MSSD monitor will compare incoming data during deployment.\n",
    "\n",
    "By default, we collect observations from **100 episodes** per environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2a. CliffWalking reference observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "!python scripts/collect_reference.py --config configs/envs/cliffwalking.yaml --agent artifacts/agents/cliffwalking/q_table.npy --output artifacts/reference/cliffwalking_ref.npz --n-episodes 100 --seed 42"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2b. CartPole reference observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "!python scripts/collect_reference.py --config configs/envs/cartpole.yaml --agent artifacts/agents/cartpole/dqn_checkpoint.pt --output artifacts/reference/cartpole_ref.npz --n-episodes 100 --seed 42"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect reference data\n",
    "\n",
    "Let's take a quick look at the reference observation shapes to make sure everything is correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "for env_name in [\"cliffwalking\", \"cartpole\"]:\n",
    "    ref_path = f\"artifacts/reference/{env_name}_ref.npz\"\n",
    "    data = np.load(ref_path)\n",
    "    obs = data[\"observations\"]\n",
    "    print(f\"{env_name}: {obs.shape[0]} observations, {obs.shape[1]}D\")\n",
    "    print(f\"  Mean: {obs.mean(axis=0).round(3)}\")\n",
    "    print(f\"  Std:  {obs.std(axis=0).round(3)}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Run Unit Tests\n",
    "\n",
    "Before running experiments, let's verify that all components (probes, martingale, metrics, wrappers, shifts) work correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m pytest tests/unit/ -v --tb=short"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All 37 unit tests should pass. If any fail, fix the underlying issue before proceeding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Smoke Test (Quick Pipeline Validation)\n",
    "\n",
    "Run a **minimal 2-trial sweep** with only body shifts to verify the entire pipeline works end-to-end before committing to the full experiment.\n",
    "\n",
    "This uses `configs/experiments/quick_smoke.yaml`, which overrides defaults for speed:\n",
    "- Only 2 trials (instead of 50)\n",
    "- Only 20 permutations (instead of 200)\n",
    "- Only 200 monitoring steps (instead of 2,000)\n",
    "- Only body shifts (instead of all 3 types)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4a. Smoke test on CliffWalking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "!python scripts/run_sweep.py --config configs/experiments/quick_smoke.yaml --agent artifacts/agents/cliffwalking/q_table.npy --reference artifacts/reference/cliffwalking_ref.npz"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4b. Smoke test on CartPole"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "!python scripts/run_sweep.py --config configs/experiments/quick_smoke.yaml --agent artifacts/agents/cartpole/dqn_checkpoint.pt --reference artifacts/reference/cartpole_ref.npz"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the smoke tests complete without errors, the pipeline is ready for the full experiments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Run a Single Trial (Optional)\n",
    "\n",
    "You can run individual trials for debugging or closer inspection. This is useful to understand what happens step-by-step.\n",
    "\n",
    "**Parameters**:\n",
    "- `--shift-type`: one of `body`, `tail`, `structure`, or `none`\n",
    "- `--severity`: shift magnitude (higher = stronger shift)\n",
    "- `--trial-id`: integer identifier for this trial\n",
    "- `--seed`: random seed for reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Example: run a single body-shift trial on CliffWalking\n!python scripts/run_experiment.py --config configs/envs/cliffwalking.yaml --defaults configs/defaults.yaml --agent artifacts/agents/cliffwalking/q_table.npy --reference artifacts/reference/cliffwalking_ref.npz --shift-type body --severity 0.6 --trial-id 0 --seed 1000 --output artifacts/results/single_trial_cliff_body.npz"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Example: run a single tail-shift trial on CartPole\n!python scripts/run_experiment.py --config configs/envs/cartpole.yaml --defaults configs/defaults.yaml --agent artifacts/agents/cartpole/dqn_checkpoint.pt --reference artifacts/reference/cartpole_ref.npz --shift-type tail --severity 1.5 --trial-id 0 --seed 2000 --output artifacts/results/single_trial_cartpole_tail.npz"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect the result\n",
    "import numpy as np\n",
    "\n",
    "result = dict(np.load(\"artifacts/results/single_trial_cliff_body.npz\", allow_pickle=True))\n",
    "print(\"Single trial result:\")\n",
    "for k, v in result.items():\n",
    "    print(f\"  {k}: {v}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. Full Experiment Sweeps\n",
    "\n",
    "Now we run the **complete experiments** required for the paper. Each sweep runs:\n",
    "\n",
    "- **50 trials** per condition\n",
    "- **3 shift types**: body, tail, structure\n",
    "- **3 severity levels** per shift type (defined in the env config)\n",
    "- **50 null (no-shift) trials** for false alarm rate estimation\n",
    "- **Total per environment**: 50 x 3 x 3 + 50 = **500 trials**\n",
    "\n",
    "The sweep runs with **4 parallel workers** by default.\n",
    "\n",
    "> **Note**: Full sweeps can take a significant amount of time depending on your hardware. The CliffWalking sweep is faster (tabular agent); the CartPole sweep is heavier (DQN inference on every step)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6a. CliffWalking full sweep\n",
    "\n",
    "Shift types and severities (from `configs/envs/cliffwalking.yaml`):\n",
    "- **Body** (coordinate offset): severities `[0.3, 0.6, 1.0]`\n",
    "- **Tail** (hazard state injection): severities `[1.0, 2.0, 3.0]`\n",
    "- **Structure** (feature decorrelation): severities `[0.5, 1.0, 2.0]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "!python scripts/run_sweep.py --config configs/experiments/sweep_cliffwalking.yaml --agent artifacts/agents/cliffwalking/q_table.npy --reference artifacts/reference/cliffwalking_ref.npz"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6b. CartPole full sweep\n",
    "\n",
    "Shift types and severities (from `configs/envs/cartpole.yaml`):\n",
    "- **Body** (pole mass drift): severities `[0.1, 0.3, 0.5]`\n",
    "- **Tail** (extreme-angle injection): severities `[1.0, 1.5, 2.0]`\n",
    "- **Structure** (x-theta correlation broken): severities `[0.3, 0.6, 1.0]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "!python scripts/run_sweep.py --config configs/experiments/sweep_cartpole.yaml --agent artifacts/agents/cartpole/dqn_checkpoint.pt --reference artifacts/reference/cartpole_ref.npz"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7. Generate Figures\n",
    "\n",
    "Once the sweeps are complete, generate the paper figures:\n",
    "\n",
    "- **Orthogonality heatmap** (3x3 matrix): shows which probe fires for each shift type. A diagonal-dominant heatmap confirms that MSSD correctly diagnoses the shift source.\n",
    "- **ADD comparison bar chart**: compares Average Detection Delay between MSSD and the global MMD baseline.\n",
    "- **Log-wealth trajectory plots**: shows how the product martingale accumulates evidence over time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7a. Figures from CliffWalking results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "!python scripts/generate_figures.py --results-dir artifacts/results/sweep_cliffwalking --output-dir artifacts/figures"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7b. Figures from CartPole results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "!python scripts/generate_figures.py --results-dir artifacts/results/sweep_cartpole --output-dir artifacts/figures"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display generated figures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "from pathlib import Path\n",
    "\n",
    "figures_dir = Path(\"artifacts/figures\")\n",
    "for fig_path in sorted(figures_dir.glob(\"*.png\")):\n",
    "    print(f\"\\n--- {fig_path.name} ---\")\n",
    "    display(Image(filename=str(fig_path), width=600))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 8. Generate LaTeX Tables\n",
    "\n",
    "Generate publication-ready LaTeX tables summarizing the results:\n",
    "- Average Detection Delay (ADD) for MSSD vs. baseline\n",
    "- Probe discrimination accuracy\n",
    "- False alarm rate (FAR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Generate tables from CliffWalking results\n!python scripts/generate_tables.py --results-dir artifacts/results/sweep_cliffwalking --output-dir artifacts/tables\n\n# Generate tables from CartPole results\n!python scripts/generate_tables.py --results-dir artifacts/results/sweep_cartpole --output-dir artifacts/tables"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the generated LaTeX table\n",
    "from pathlib import Path\n",
    "\n",
    "for tex_path in sorted(Path(\"artifacts/tables\").glob(\"*.tex\")):\n",
    "    print(f\"\\n=== {tex_path.name} ===\")\n",
    "    print(tex_path.read_text())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 9. Quick Results Summary (In-Notebook Analysis)\n",
    "\n",
    "For a quick look at the results directly in the notebook, we can load and analyze them using the built-in evaluation module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from mssd.evaluation.metrics import TrialResult, compute_add, compute_far, compute_discrimination_accuracy\n",
    "\n",
    "def load_results(results_dir):\n",
    "    \"\"\"Load all .npz result files into TrialResult objects.\"\"\"\n",
    "    results = []\n",
    "    for npz_path in Path(results_dir).rglob(\"*.npz\"):\n",
    "        data = dict(np.load(npz_path, allow_pickle=True))\n",
    "        alarm_step = int(data[\"mssd_alarm_step\"])\n",
    "        baseline_step = int(data[\"baseline_alarm_step\"])\n",
    "        results.append(TrialResult(\n",
    "            env_name=str(data[\"env_name\"]),\n",
    "            shift_type=str(data[\"shift_type\"]),\n",
    "            severity=float(data[\"severity\"]),\n",
    "            trial_id=int(data[\"trial_id\"]),\n",
    "            seed=int(data[\"seed\"]),\n",
    "            mssd_alarm_fired=bool(data[\"mssd_alarm_fired\"]),\n",
    "            mssd_alarm_step=alarm_step if alarm_step >= 0 else None,\n",
    "            mssd_diagnosed_probe=str(data[\"mssd_diagnosed_probe\"]) if str(data[\"mssd_diagnosed_probe\"]) != \"none\" else None,\n",
    "            mssd_log_wealth={},\n",
    "            baseline_alarm_fired=bool(data[\"baseline_alarm_fired\"]),\n",
    "            baseline_alarm_step=baseline_step if baseline_step >= 0 else None,\n",
    "            shift_injection_step=int(data[\"shift_injection_step\"]),\n",
    "            total_steps=int(data[\"total_steps\"]),\n",
    "        ))\n",
    "    return results\n",
    "\n",
    "\n",
    "for env in [\"sweep_cliffwalking\", \"sweep_cartpole\"]:\n",
    "    results_dir = f\"artifacts/results/{env}\"\n",
    "    if not Path(results_dir).exists():\n",
    "        print(f\"No results found for {env} - run the sweep first.\")\n",
    "        continue\n",
    "    \n",
    "    results = load_results(results_dir)\n",
    "    if not results:\n",
    "        print(f\"No results found in {results_dir}\")\n",
    "        continue\n",
    "    \n",
    "    shift_results = [r for r in results if r.shift_type != \"none\"]\n",
    "    null_results = [r for r in results if r.shift_type == \"none\"]\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"  {env.upper()}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Total trials: {len(results)} ({len(shift_results)} shifted, {len(null_results)} null)\")\n",
    "    \n",
    "    # Detection rate\n",
    "    if shift_results:\n",
    "        detected = sum(1 for r in shift_results if r.mssd_alarm_fired)\n",
    "        print(f\"Detection rate: {detected}/{len(shift_results)} = {detected/len(shift_results):.1%}\")\n",
    "        \n",
    "        # ADD\n",
    "        add = compute_add(shift_results)\n",
    "        print(f\"Average Detection Delay (MSSD): {add:.1f} steps\")\n",
    "    \n",
    "    # FAR\n",
    "    if null_results:\n",
    "        far = compute_far(null_results)\n",
    "        print(f\"False Alarm Rate: {far:.3f} (target: < 0.05)\")\n",
    "    \n",
    "    # Discrimination accuracy per shift type\n",
    "    if shift_results:\n",
    "        for shift_type in [\"body\", \"tail\", \"structure\"]:\n",
    "            typed = [r for r in shift_results if r.shift_type == shift_type]\n",
    "            if typed:\n",
    "                acc = compute_discrimination_accuracy(typed, shift_type)\n",
    "                print(f\"  {shift_type} discrimination accuracy: {acc:.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "### Expected outcomes\n",
    "\n",
    "If everything works correctly, you should see:\n",
    "\n",
    "1. **Orthogonality**: The 3x3 heatmap should be **diagonal-dominant**, meaning each probe primarily fires for its corresponding shift type (body probe detects body shifts, tail probe detects tail shifts, structure probe detects structure shifts).\n",
    "\n",
    "2. **Low false alarm rate**: FAR should be below the significance level alpha = 0.05.\n",
    "\n",
    "3. **Competitive detection delay**: MSSD should achieve ADD comparable to or better than the global MMD baseline, while additionally providing a **diagnosis** of the shift type.\n",
    "\n",
    "4. **Severity monotonicity**: Higher severity shifts should be detected faster (lower ADD).\n",
    "\n",
    "### Troubleshooting\n",
    "\n",
    "- **Agent training fails**: Check that gymnasium is installed and the env name is correct (`CliffWalking-v1`, `CartPole-v1`).\n",
    "- **GPU not detected**: Ensure your conda env has CUDA-enabled PyTorch. Run `torch.cuda.is_available()` to check.\n",
    "- **Sweep takes too long**: Reduce `n_trials` in the sweep config, or increase `parallel_workers`.\n",
    "- **Tests fail**: Run `pytest tests/unit/ -v` and fix any failures before proceeding."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_5070",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}